# ğŸš€ Sales Intelligence - BigQuery + BigQuery ML Architecture

## ğŸ“‹ VisÃ£o Geral

Esta Ã© a arquitetura "Endgame" do Sales Intelligence Engine, usando:
- **BigQuery**: Data Warehouse centralizado para armazenar todos os dados de vendas
- **BigQuery ML**: Machine Learning nativo para prediÃ§Ã£o de Win/Loss
- **Cloud Functions**: Engine de anÃ¡lise e orquestraÃ§Ã£o
- **Apps Script**: Interface com Google Sheets

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Sheets  â”‚
â”‚  (Apps Script)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1. Load Data
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BigQuery                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Dataset: sales_intelligenceâ”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚ Tables:                    â”‚  â”‚
â”‚  â”‚  â€¢ pipeline                â”‚  â”‚
â”‚  â”‚  â€¢ closed_deals            â”‚  â”‚
â”‚  â”‚  â€¢ pipeline_predictions    â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚ ML Models:                 â”‚  â”‚
â”‚  â”‚  â€¢ win_loss_predictor      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 2. Query & Analyze
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Function  â”‚
â”‚  (Python 3.12)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 3. Return Results
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Sheets  â”‚
â”‚   (Dashboard)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Estrutura de Arquivos

```
/workspaces/playbook/
â”œâ”€â”€ bigquery/
â”‚   â”œâ”€â”€ schema_pipeline.json           # Schema da tabela pipeline
â”‚   â”œâ”€â”€ schema_closed.json             # Schema da tabela closed_deals
â”‚   â”œâ”€â”€ setup_bigquery.sh              # Script de setup inicial
â”‚   â”œâ”€â”€ load_initial_data.py           # Carrega CSVs iniciais
â”‚   â””â”€â”€ ml_win_loss_model.sql          # Modelo de ML Win/Loss
â”œâ”€â”€ appscript/
â”‚   â”œâ”€â”€ BigQueryLoader.gs              # Carrega dados no BigQuery
â”‚   â”œâ”€â”€ DashboardCode.gs               # Dashboard principal
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cloud-function/
â”‚   â”œâ”€â”€ main_bigquery.py               # Cloud Function (versÃ£o BigQuery)
â”‚   â””â”€â”€ requirements.txt               # DependÃªncias Python
â””â”€â”€ *.csv                              # Dados atuais (para carga inicial)
```

## ğŸš€ Deployment - Passo a Passo

### PrÃ©-requisitos

1. **AutenticaÃ§Ã£o no Google Cloud**
   ```bash
   cd /workspaces/playbook
   gcloud auth login
   gcloud config set project operaciones-br
   ```

### Etapa 1: Setup do BigQuery

```bash
cd /workspaces/playbook/bigquery

# 1. Criar dataset e tabelas
./setup_bigquery.sh

# 2. Carregar dados iniciais dos CSVs
./load_initial_data.py
```

**O que acontece:**
- âœ… Dataset `sales_intelligence` criado
- âœ… Tabela `pipeline` criada com particionamento por data
- âœ… Tabela `closed_deals` criada com particionamento por data
- âœ… ~270 linhas de pipeline carregadas
- âœ… ~2575 linhas de closed deals carregadas (506 WON + 2069 LOST)

**Verificar resultado:**
```bash
bq show operaciones-br:sales_intelligence
bq query --use_legacy_sql=false \
  "SELECT COUNT(*) as total FROM \`operaciones-br.sales_intelligence.pipeline\`"
```

### Etapa 2: Criar Modelo de ML

```bash
# Executar o SQL de criaÃ§Ã£o do modelo
bq query --use_legacy_sql=false < ml_win_loss_model.sql
```

**O que acontece:**
- âœ… View `training_data` criada (features engenheiradas)
- âœ… Modelo `win_loss_predictor` treinado (XGBoost)
- âœ… MÃ©tricas de avaliaÃ§Ã£o calculadas
- âœ… Tabela `pipeline_predictions` criada com probabilidades

**Tempo estimado:** 3-5 minutos para treinar o modelo

**Verificar resultado:**
```bash
bq query --use_legacy_sql=false \
  "SELECT * FROM ML.EVALUATE(MODEL \`operaciones-br.sales_intelligence.win_loss_predictor\`)"
```

### Etapa 3: Deploy da Cloud Function

```bash
cd /workspaces/playbook/cloud-function

# Copiar versÃ£o BigQuery como main.py
cp main_bigquery.py main.py

# Deploy
gcloud functions deploy sales-intelligence-engine \
  --gen2 \
  --runtime=python312 \
  --region=us-central1 \
  --source=. \
  --entry-point=sales_intelligence_engine \
  --trigger-http \
  --allow-unauthenticated \
  --memory=2GB \
  --timeout=540s
```

**O que acontece:**
- âœ… Cloud Function atualizada para ler do BigQuery
- âœ… Biblioteca `google-cloud-bigquery` instalada
- âœ… Endpoint HTTP disponÃ­vel

**Verificar resultado:**
```bash
# Testar com curl
curl -X POST https://us-central1-operaciones-br.cloudfunctions.net/sales-intelligence-engine \
  -H "Content-Type: application/json" \
  -d '{"source": "bigquery", "filters": {}}'
```

### Etapa 4: Configurar Apps Script

1. **Abrir o Google Sheets** com seus dados

2. **Abrir Editor de Scripts** (Extensions > Apps Script)

3. **Adicionar Biblioteca BigQuery**:
   - Resources > Libraries
   - Script ID: `1JefJJw2F7kd5ykBlF_yFmQ8AJkz3GhCvUYKlv4wWQbfQwkJLnM4xNnqV`
   - VersÃ£o: `36` (ou mais recente)
   - Identifier: `BigQuery`

4. **Criar novo arquivo** `BigQueryLoader.gs`:
   - Copiar conteÃºdo de `/workspaces/playbook/appscript/BigQueryLoader.gs`
   - Colar no Apps Script

5. **Salvar e autorizar**:
   - Salvar o projeto
   - Executar funÃ§Ã£o `runFullAnalysis()`
   - Autorizar acesso ao BigQuery

### Etapa 5: Executar Primeira AnÃ¡lise

No Apps Script, execute:

```javascript
function testFullPipeline() {
  Logger.log('ğŸš€ Testando pipeline completo...');
  
  // 1. Carregar dados no BigQuery
  Logger.log('\n[1/2] Carregando dados...');
  loadPipelineToBigQuery();
  loadClosedDealsToBigQuery();
  
  // 2. Executar anÃ¡lise
  Logger.log('\n[2/2] Executando anÃ¡lise...');
  const result = callCloudFunctionWithBigQuery({});
  
  Logger.log('\nâœ… Resultado:');
  Logger.log(JSON.stringify(result, null, 2));
}
```

## ğŸ“Š Usando o Modelo de ML

### Query 1: Top Oportunidades em Risco

```sql
SELECT
  oportunidade,
  conta,
  vendedor,
  gross,
  ROUND(win_probability * 100, 1) as win_prob_pct,
  ml_alert,
  fase_atual
FROM `operaciones-br.sales_intelligence.pipeline_predictions`
WHERE win_probability < 0.5
  AND gross > 50000
ORDER BY gross DESC
LIMIT 10;
```

### Query 2: Performance por Vendedor (PrediÃ§Ã£o)

```sql
SELECT
  vendedor,
  COUNT(*) as num_deals,
  ROUND(AVG(win_probability) * 100, 1) as avg_win_prob,
  SUM(gross) as total_pipeline,
  SUM(CASE WHEN win_probability > 0.5 THEN gross ELSE 0 END) as likely_value
FROM `operaciones-br.sales_intelligence.pipeline_predictions`
GROUP BY vendedor
ORDER BY avg_win_prob DESC;
```

### Query 3: Forecast Accuracy (ML vs. Manual)

```sql
SELECT
  forecast_ia,
  COUNT(*) as num_deals,
  ROUND(AVG(win_probability) * 100, 1) as avg_ml_prediction,
  SUM(gross) as total_value
FROM `operaciones-br.sales_intelligence.pipeline_predictions`
GROUP BY forecast_ia
ORDER BY avg_ml_prediction DESC;
```

## ğŸ”„ Fluxo de Uso DiÃ¡rio

### OpÃ§Ã£o A: AtualizaÃ§Ã£o Manual (Apps Script)

1. Abrir Google Sheets
2. Executar funÃ§Ã£o: `runFullAnalysis()`
3. Aguardar 10-30 segundos
4. Visualizar resultados no Dashboard

### OpÃ§Ã£o B: AtualizaÃ§Ã£o AutomÃ¡tica (Trigger)

```javascript
// No Apps Script, criar trigger:
function createDailyTrigger() {
  ScriptApp.newTrigger('runFullAnalysis')
    .timeBased()
    .everyDays(1)
    .atHour(8)  // 8 AM
    .create();
}
```

### OpÃ§Ã£o C: Retreinar Modelo (Semanal)

```bash
# Reexecutar apenas as partes 2, 3, 4, 5 do SQL
bq query --use_legacy_sql=false < ml_win_loss_model.sql
```

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Teste 1: Verificar Dados no BigQuery

```bash
bq query --use_legacy_sql=false "
SELECT 
  'pipeline' as table_name,
  COUNT(*) as rows,
  MAX(data_carga) as last_load
FROM \`operaciones-br.sales_intelligence.pipeline\`

UNION ALL

SELECT 
  'closed_deals' as table_name,
  COUNT(*) as rows,
  MAX(data_carga) as last_load
FROM \`operaciones-br.sales_intelligence.closed_deals\`
"
```

### Teste 2: Verificar Modelo de ML

```bash
bq query --use_legacy_sql=false "
SELECT
  accuracy,
  precision,
  recall,
  f1_score,
  log_loss,
  roc_auc
FROM ML.EVALUATE(MODEL \`operaciones-br.sales_intelligence.win_loss_predictor\`)
"
```

**MÃ©tricas esperadas:**
- Accuracy: > 70%
- ROC AUC: > 0.75
- Precision/Recall: Balanceados

### Teste 3: Verificar Cloud Function

```bash
curl -X POST https://us-central1-operaciones-br.cloudfunctions.net/sales-intelligence-engine \
  -H "Content-Type: application/json" \
  -d '{
    "source": "bigquery",
    "filters": {
      "quarter": "FY26-Q1"
    }
  }' | jq .
```

**Resposta esperada:**
```json
{
  "status": "success",
  "data_summary": {
    "pipeline_deals": 150,
    "closed_deals": 500,
    "sellers_analyzed": 5
  },
  "pipeline_analysis": { ... },
  "closed_analysis": {
    "win_rate": 45.2,
    ...
  }
}
```

## ğŸ“ˆ Vantagens desta Arquitetura

### âœ… Performance
- **Antes**: 6.4 MB de payload, timeout em requisiÃ§Ãµes HTTP
- **Depois**: Query otimizada, resultados em < 2 segundos

### âœ… Escalabilidade
- **Antes**: Limitado a ~3000 deals
- **Depois**: Suporta milhÃµes de linhas sem alteraÃ§Ã£o de cÃ³digo

### âœ… InteligÃªncia
- **Antes**: AnÃ¡lise descritiva (o que aconteceu)
- **Depois**: AnÃ¡lise preditiva (o que vai acontecer)

### âœ… HistÃ³rico
- **Antes**: Apenas snapshot atual
- **Depois**: AnÃ¡lise histÃ³rica (Quarter-over-Quarter, Year-over-Year)

### âœ… Insights de ML
- Probabilidade real de vitÃ³ria por deal
- IdentificaÃ§Ã£o automÃ¡tica de deals em risco
- ComparaÃ§Ã£o de performance entre vendedores
- Causas de perda mais frequentes

## ğŸ”® PrÃ³ximos Passos (Opcional)

### Deep Learning para AnÃ¡lise de Texto

Se quiser analisar os campos de texto (`resumo_analise`, `licoes_aprendidas`):

```sql
-- Criar modelo de DNN para anÃ¡lise de sentimento/causa
CREATE OR REPLACE MODEL `operaciones-br.sales_intelligence.loss_cause_predictor`
OPTIONS(
  model_type='DNN_CLASSIFIER',
  input_label_cols=['causa_raiz'],
  hidden_units=[128, 64, 32]
) AS
SELECT
  causa_raiz,
  resumo_analise,
  licoes_aprendidas,
  competidor,
  gross,
  ciclo_dias
FROM `operaciones-br.sales_intelligence.closed_deals`
WHERE outcome = 'LOST'
  AND causa_raiz IS NOT NULL;
```

### IntegraÃ§Ã£o com Looker Studio

1. Conectar Looker Studio ao BigQuery
2. Criar dashboard visual com:
   - Pipeline por probabilidade de vitÃ³ria
   - Top deals em risco
   - Performance por vendedor
   - TendÃªncias histÃ³ricas

### Alertas AutomÃ¡ticos

Criar Cloud Scheduler para enviar alertas via email quando:
- Deal de alto valor cai abaixo de 30% de probabilidade
- Vendedor tem win rate < 25%
- Pipeline de Q nÃ£o vai bater a meta

## ğŸ†˜ Troubleshooting

### Erro: "Permission denied on BigQuery"
```bash
# Conceder permissÃµes Ã  Cloud Function
gcloud projects add-iam-policy-binding operaciones-br \
  --member="serviceAccount:operaciones-br@appspot.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"
```

### Erro: "Table not found"
```bash
# Verificar se as tabelas existem
bq ls operaciones-br:sales_intelligence
```

### Erro: "Model training failed"
```bash
# Ver logs de treinamento
bq show -j <job_id>
```

## ğŸ“š ReferÃªncias

- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [BigQuery ML Guide](https://cloud.google.com/bigquery-ml/docs/introduction)
- [Apps Script BigQuery Service](https://developers.google.com/apps-script/advanced/bigquery)
- [Cloud Functions Python](https://cloud.google.com/functions/docs/writing)
