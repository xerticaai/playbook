#!/usr/bin/env python3
"""
Enriquece closed_deals com anÃ¡lises de ganhas e perdidas.
Separa em 2 tabelas: closed_deals_won e closed_deals_lost.

USO:
    python3 enrich_closed_deals.py
"""

import pandas as pd
from google.cloud import bigquery
import sys

PROJECT_ID = "operaciones-br"
DATASET_ID = "sales_intelligence"

def load_csv_ganhas():
    """Carrega CSV de anÃ¡lise de ganhas"""
    print("ğŸ“Š Carregando CSV de ganhas...")
    
    df = pd.read_csv("../Forecast 2026 - Base  - ğŸ“ˆ AnÃ¡lise Ganhas.csv", encoding='utf-8')
    
    print(f"   âœ… {len(df)} deals ganhos carregados")
    print(f"   ğŸ“‹ Colunas: {len(df.columns)}")
    
    # Adiciona outcome
    df['outcome'] = 'WON'
    
    return df

def load_csv_perdidas():
    """Carrega CSV de anÃ¡lise de perdidas"""
    print("ğŸ“Š Carregando CSV de perdidas...")
    
    df = pd.read_csv("../Forecast 2026 - Base  - ğŸ“‰ AnÃ¡lise Perdidas.csv", encoding='utf-8')
    
    print(f"   âœ… {len(df)} deals perdidos carregados")
    print(f"   ğŸ“‹ Colunas: {len(df.columns)}")
    
    # Adiciona outcome
    df['outcome'] = 'LOST'
    
    return df

def standardize_columns(df, outcome='WON'):
    """Padroniza nomes de colunas"""
    
    # Renomear colunas comuns
    rename_map = {
        'Run ID': 'Run_ID',
        'Perfil Cliente': 'Perfil_Cliente',
        'Fiscal Q': 'Fiscal_Q',
        'Data Fechamento': 'Data_Fechamento',
        'Ciclo (dias)': 'Ciclo_dias',
        'ğŸ“ Resumo AnÃ¡lise': 'Resumo_Analise',
        'ğŸ¯ Causa Raiz': 'Causa_Raiz',
        'Tipo Resultado': 'Tipo_Resultado',
        'Qualidade Engajamento': 'Qualidade_Engajamento',
        'GestÃ£o Oportunidade': 'Gestao_Oportunidade',
        'ğŸ’¡ LiÃ§Ãµes Aprendidas': 'Licoes_Aprendidas',
        '# Atividades': 'Atividades',
        'Ativ. 7d': 'Ativ_7d',
        'Ativ. 30d': 'Ativ_30d',
        'DistribuiÃ§Ã£o Tipos': 'Distribuicao_Tipos',
        'PerÃ­odo Pico': 'Periodo_Pico',
        'CadÃªncia MÃ©dia (dias)': 'Cadencia_Media_dias',
        '# Total MudanÃ§as': 'Total_Mudancas',
        '# MudanÃ§as CrÃ­ticas': 'Mudancas_Criticas',
        'MudanÃ§as Close Date': 'Mudancas_Close_Date',
        'MudanÃ§as Stage': 'Mudancas_Stage',
        'MudanÃ§as Valor': 'Mudancas_Valor',
        'Campos + Alterados': 'Campos_Alterados',
        'PadrÃ£o MudanÃ§as': 'Padrao_Mudancas',
        'Freq. MudanÃ§as': 'Freq_Mudancas',
        '# Editores': 'Editores',
        'ğŸ·ï¸ Labels': 'Labels',
        'ğŸ• Ãšltima AtualizaÃ§Ã£o': 'Ultima_Atualizacao',
        'FamÃ­lia Produto': 'Familia_Produto',
        'PortfÃ³lio': 'Portfolio'
    }
    
    # Renomear especÃ­ficos de ganhas
    if outcome == 'WON':
        rename_map.update({
            'âœ¨ Fatores Sucesso': 'Fatores_Sucesso'
        })
    
    # Renomear especÃ­ficos de perdidas
    if outcome == 'LOST':
        rename_map.update({
            'âš ï¸ Causas SecundÃ¡rias': 'Causas_Secundarias',
            'EvitÃ¡vel?': 'Evitavel',
            'ğŸš¨ Sinais Alerta': 'Sinais_Alerta',
            'Momento CrÃ­tico': 'Momento_Critico'
        })
    
    df = df.rename(columns=rename_map)
    
    return df

def upload_to_bigquery(df, table_name):
    """Upload DataFrame para BigQuery"""
    print(f"\nğŸ“¤ Uploading para {table_name}...")
    
    client = bigquery.Client(project=PROJECT_ID)
    table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
    
    # ConfiguraÃ§Ã£o de job
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE",  # Sobrescreve
        autodetect=True,  # Auto-detecta schema
        source_format=bigquery.SourceFormat.CSV
    )
    
    # Upload
    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
    job.result()  # Wait for completion
    
    # Verificar
    table = client.get_table(table_id)
    print(f"   âœ… {table.num_rows} linhas carregadas")
    print(f"   ğŸ“Š {len(table.schema)} colunas")
    
    return table

def main():
    print("=" * 80)
    print("ğŸ”„ ENRIQUECIMENTO DE CLOSED DEALS")
    print("=" * 80)
    print()
    
    # 1. Carregar CSVs
    df_ganhas = load_csv_ganhas()
    df_perdidas = load_csv_perdidas()
    
    print()
    
    # 2. Padronizar colunas
    print("ğŸ”§ Padronizando colunas...")
    df_ganhas = standardize_columns(df_ganhas, outcome='WON')
    df_perdidas = standardize_columns(df_perdidas, outcome='LOST')
    print("   âœ… Colunas padronizadas")
    
    print()
    
    # 3. Ver sample de campos importantes
    print("ğŸ“‹ SAMPLE DE CAMPOS IMPORTANTES (Ganhas):")
    print(f"   - Causa_Raiz: {df_ganhas['Causa_Raiz'].iloc[0][:80]}...")
    if 'Fatores_Sucesso' in df_ganhas.columns:
        print(f"   - Fatores_Sucesso: {df_ganhas['Fatores_Sucesso'].iloc[0][:80]}...")
    print(f"   - Tipo_Resultado: {df_ganhas['Tipo_Resultado'].iloc[0]}")
    print(f"   - Qualidade_Engajamento: {df_ganhas['Qualidade_Engajamento'].iloc[0]}")
    print(f"   - Atividades: {df_ganhas['Atividades'].iloc[0]}")
    print(f"   - Ciclo_dias: {df_ganhas['Ciclo_dias'].iloc[0]}")
    
    print()
    
    print("ğŸ“‹ SAMPLE DE CAMPOS IMPORTANTES (Perdidas):")
    print(f"   - Causa_Raiz: {df_perdidas['Causa_Raiz'].iloc[0][:80]}...")
    if 'Causas_Secundarias' in df_perdidas.columns:
        print(f"   - Causas_Secundarias: {df_perdidas['Causas_Secundarias'].iloc[0][:80]}..." if pd.notna(df_perdidas['Causas_Secundarias'].iloc[0]) else "   - Causas_Secundarias: (empty)")
    if 'Evitavel' in df_perdidas.columns:
        print(f"   - Evitavel: {df_perdidas['Evitavel'].iloc[0]}")
    print(f"   - Tipo_Resultado: {df_perdidas['Tipo_Resultado'].iloc[0]}")
    print(f"   - Atividades: {df_perdidas['Atividades'].iloc[0]}")
    print(f"   - Ciclo_dias: {df_perdidas['Ciclo_dias'].iloc[0]}")
    
    print()
    
    # 4. Upload para BigQuery
    table_won = upload_to_bigquery(df_ganhas, "closed_deals_won")
    table_lost = upload_to_bigquery(df_perdidas, "closed_deals_lost")
    
    print()
    print("=" * 80)
    print("ğŸ‰ ENRIQUECIMENTO COMPLETO!")
    print("=" * 80)
    print()
    print("ğŸ“Š RESUMO:")
    print(f"   âœ… closed_deals_won: {table_won.num_rows} deals, {len(table_won.schema)} campos")
    print(f"   âœ… closed_deals_lost: {table_lost.num_rows} deals, {len(table_lost.schema)} campos")
    print()
    print("ğŸ”— PRÃ“XIMOS PASSOS:")
    print("   1. Treinar modelos ML com dados enriquecidos")
    print("   2. Atualizar BigQuerySync para salvar em tabelas separadas")
    print("   3. Criar views unificadas se necessÃ¡rio")
    print()
    print("ğŸ“ QUERIES DE TESTE:")
    print(f"   bq query 'SELECT Causa_Raiz, COUNT(*) FROM `{PROJECT_ID}.{DATASET_ID}.closed_deals_won` GROUP BY 1 LIMIT 5'")
    print(f"   bq query 'SELECT Causa_Raiz, COUNT(*) FROM `{PROJECT_ID}.{DATASET_ID}.closed_deals_lost` GROUP BY 1 LIMIT 5'")
    print()

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ ERRO: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
